@InProceedings{fevzullah2004taylor,
    author={Temurtas, Fevzullah and Gulbag, Ali and Yumusak, Nejat},
    editor={Lagana, Antonio and Gavrilova, Marina L. and Kumar, Vipin and Mun, Youngsong and Tan, C. J. Kenneth and Gervasi, Osvaldo},
    title={A Study on Neural Networks Using Taylor Series Expansion of Sigmoid Activation Function},
    booktitle={Computational Science and Its Applications},
    year={2004},
    publisher={Springer Berlin Heidelberg},
    address={Berlin, Heidelberg},
    pages={389--397},
    abstract={The use of microcontroller in neural network realizations is cheaper than those specific neural chips. However, realization of complicated mathematical operations such as sigmoid activation function is difficult via general microcontrollers. On the other hand, it is possible to make approximation to the sigmoid activation function. In this study, Taylor series expansions up to nine terms are used to realize sigmoid activation function. The neural network (NN) structures with Taylor series expansions of sigmoid activation function are used for the concentration estimation of Toluene gas from the trend of the transient sensor responses. The Quartz Crystal Microbalance (QCM) type sensors were used as gas sensors. The appropriateness of the NNs for the gas concentration determination inside the sensor response time is observed with five different terms of Taylor series expansion.},
    isbn={978-3-540-24768-5}
}

@misc{paszke2019pytorch,
    title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
    author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
    year={2019},
    eprint={1912.01703},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{li2021effectiveness,
    title={On the Effectiveness of Interpretable Feedforward Neural Network}, 
    author={Miles Q. Li and Benjamin C. M. Fung and Adel Abusitta},
    year={2021},
    eprint={2111.02303},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{bebis1994nns,  
    author={Bebis, G. and Georgiopoulos, M.},  
    journal={IEEE Potentials},   
    title={Feed-forward neural networks},   
    year={1994},  
    volume={13},  
    number={4},  
    pages={27-31},  
    doi={10.1109/45.329294}
}

@article{murat2006overview,
    author = {Sazli, Murat},
    year = {2006},
    month = {01},
    pages = {11-17},
    title = {A brief review of feed-forward neural networks},
    volume = {50},
    journal = {Communications, Faculty Of Science, University of Ankara},
    doi = {10.1501/0003168}
}

@article{Jang2008NeuralNI,
    title={Neural Network Implementation Using CUDA and OpenMP},
    author={Honghoon Jang and Anjin Park and Keechul Jung},
    journal={2008 Digital Image Computing: Techniques and Applications},
    year={2008},
    pages={155-161}
}

@article{huqqani2013multicore,
    title = {Multicore and GPU Parallelization of Neural Networks for Face Recognition},
    journal = {Procedia Computer Science},
    volume = {18},
    pages = {349-358},
    year = {2013},
    note = {2013 International Conference on Computational Science},
    issn = {1877-0509},
    doi = {https://doi.org/10.1016/j.procs.2013.05.198},
    url = {https://www.sciencedirect.com/science/article/pii/S1877050913003414},
    author = {Altaf Ahmad Huqqani and Erich Schikuta and Sicen Ye and Peng Chen},
    keywords = {Parallel Simulation, Multicores, GPUs, OpenMP, CUDA, Artificial Neural Network},
    abstract = {Training of Artificial Neural Networks for large data sets is a time consuming task. Various approaches have been proposed to reduce the efforts, many of them by applying parallelization techniques. In this paper we develop and analyze two novel parallel training approaches for Backpropagation neural networks for face recognition. We focus on two specific paralleliza- tion environments, using on the one hand OpenMP on a conventional multithreaded CPU and CUDA on a GPU. Based on our findings we give guidelines for the efficient parallelization of Backpropagation neural networks on multicore and GPU architectures. Additionally, we present a traversal method finding the best combination of learning rate and momentum term by varying the number of hidden neurons supporting the parallelization efforts.}
}

@book{stourstrup2013cpp,
    author = {Stroustrup, Bjarne},
    title = {The C++ Programming Language},
    year = {2013},
    isbn = {0321563840},
    publisher = {Addison-Wesley Professional},
    edition = {4th},
    abstract = {C++11 has arrived: thoroughly master it, with the definitive new guide from C++ creator Bjarne Stroustrup, C++ Programming Language, Fourth Edition! The brand-new edition of the world's most trusted and widely read guide to C++, it has been comprehensively updated for the long-awaited C++11 standard. Extensively rewritten to present the C++11 language, standard library, and key design techniques as an integrated whole, Stroustrup thoroughly addresses changes that make C++11 feel like a whole new language, offering definitive guidance for leveraging its improvements in performance, reliability, and clarity. C++ programmers around the world recognize Bjarne Stoustrup as the go-to expert for the absolutely authoritative and exceptionally useful information they need to write outstanding C++ programs. Now, as C++11 compilers arrive and development organizations migrate to the new standard, they know exactly where to turn once more: Stoustrup's C++ Programming Language, Fourth Edition.}
}

@article{dagum1998openmp,
    author = {Dagum, Leonardo and Menon, Ramesh},
    title = {OpenMP: An Industry-Standard API for Shared-Memory Programming},
    year = {1998},
    issue_date = {January 1998},
    publisher = {IEEE Computer Society Press},
    address = {Washington, DC, USA},
    volume = {5},
    number = {1},
    issn = {1070-9924},
    url = {https://doi.org/10.1109/99.660313},
    doi = {10.1109/99.660313},
    abstract = {The authors present a new way to achieve scalability in parallel software with OpenMP, their portable alternative to message passing. They discuss its capabilities through specific examples and comparisons with other standard parallel programming models.},
    journal = {IEEE Comput. Sci. Eng.},
    month = {jan},
    pages = {46-55},
    numpages = {10}
}

@misc{cuda,
    author={NVIDIA and Vingelmann, Péter and Fitzek, Frank H.P.},
    title={CUDA, release: 10.2.89},
    year={2020},
    url={https://developer.nvidia.com/cuda-toolkit},
}

@Article{Cowan1990McCulloch,
    author={Cowan, Jack D.},
    title={Discussion: McCulloch-Pitts and related neural nets from 1943 to 1989},
    journal={Bulletin of Mathematical Biology},
    year={1990},
    month={Jan},
    day={01},
    volume={52},
    number={1},
    pages={73-97},
    abstract={The McCulloch-Pitts paper ``A Logical Calculus of the Ideas Immanent in Nervous Activity'' was published in theBulletin of Mathematical Biophysics in 1943, a decade before the work of Hodgkin, Huxley, Katz and Eccles. The McCulloch-Pitts neuron is an extremely simplified representation of neural properties, based simply on the existence of a threshold for the activation of an action potential.},
    issn={1522-9602},
    doi={10.1007/BF02459569},
    url={https://doi.org/10.1007/BF02459569}
}

@article{FashionMNIST2017Xiao,
  author    = {Han Xiao and
               Kashif Rasul and
               Roland Vollgraf},
  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
               Algorithms},
  journal   = {CoRR},
  volume    = {abs/1708.07747},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.07747},
  eprinttype = {arXiv},
  eprint    = {1708.07747},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-07747.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

